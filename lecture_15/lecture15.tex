\iffalse
Supervised Learning for Fraud Detection
    Assumes the availability of historical labelled data, so it can identify known fraudulent patterns.

    Main problem: usually the labels related to transactions are difficult to obtain
        because they are not provided by the entity with which you're collaborating
        and usually the lables are not noise free (and we don't want to learn noise because it will make us overfit)

    Regression vs Classification 
    
    In regression target variable continous and varies along a predef. interval
    In classification target variable is categorical (binary class vs multi-class class.)

    Linear Regression 
        Technique that tries to model a continous target value
        General formulation 
        y(x,w) = w0 + w1x1 + \dots
        y is the target variable 
        w are the parameters 
        x are the explanatory variables (features)

        weight the features based on their important
        we're training the weights 
        how do you train them?
            minimize a cost function (loss function)
            minimize the Squared Error Function 
            try to minimize the error of the prediction wrt the values that compose the dataset
            try to find the best Line(linear relation) that represent the dataset
            w0 is the intercept parameter of the line wrt the y axis 

    Logistic Regression (Classification)
        When you have a categorical target variable 
        If you try to apply linear regression, the thing is shit because
        the error/target are not normally distributed but follow a bernoulli distribution
        No guarantees that the linear regression target is between 0 and 1

        You want a function that stays in between 0 and 1 (Use S-Curved )
        You're bounding the values of the target variable 

        We decide to fit with a sigmoid function, the parameters are now put as 
        exponents of an exponential in the denominator of a fraction 

        You can interpret log. regression result as probabilities 

        We pass the weighted sum of the inputs through an activation function that 
        can map the values in between 0 and 1 
        The activation function is called sigmoid 
        Optimized with MLE 

        From higher perspective logistic regression tries to estimate the best plane where to separate both classes 
        Set a treshold and decide which parameters are considered okay or fraudulent 

        Used for:
            classification and/or regression 
            output between 0 and 1 

    Linear and Logistic: variable selection 
        built in procedures to perform variable selection 
        Linear regression --> perfrom statistical test based on Student's t 
        Logistic regression --> perform based on Chi-squared distribution 

        p value 
        A low p-value means significant variable 
        High p-value means insignificant variable 

        Perform these:
            forward regression --> start from few features and add 
            backward regression 
            stepwise regression 

        Evaluation criteria for variable selection 
            statistical significance (p-value)
            interpretability
                sign of the regression coefficient: direction of 
                    positive: 
                    negative: 
            Operational efficiency 
            Legal issues: some variables cannot be used because of privacy or discrimination concers 

    Slide 144 Recap

    Decision Trees
        recursive partitioning algo built with tree-like structure 

        top node= root node where you specify a testing condition of which the outcome corresponds to a branch leading up to an internal node 
        leaf nodes assign fraud labels 

        Implement 3 main funcitonalities 
            splitting decision : 
            stopping decision 
            assignment decision : to which class assign a specific instance of the dataset 

            Splitting Decision:
                to understand it we define the concepts of impurity and chaos
                with an example of three boxes with two different objects 
                minimal impurity= all the boxes contains the same object (o black or white per ciascuno)
                maximal impurity = when you have a box that contain an equal number of good or bad 

                Impurity measures:
                    entropy: bounded in 0 and 1
                    gini : bounded in 0 and 0.5
                    you can use both with no restrictions 

                By the end of the day the algortim selects a sequence of candidates for a splitting decision and
                then selects the one that minimizes the impurity 

                We measure the decrease of impurity with the gain 
                higher gain is better

                The algo can perfectly be parallelized and executed with a greedy and recursive strategy

            Stopping decision:
                ha detto delle cose alle 12:59

            Decision Tree Properties
                binary trees:
                multiple branches:
                difference is the shape of the tree

                decision trees are interpretable models, you can directly extract rule sets.

                By building these and automatically extract rules you have a tool which can help 
                experts in build rules --> no more manual update

            Decision Boundaries
                the tree decides to build more decision boundaries to split data 
                manca ragionamento

            Using Decision Trees in Fraud Analytics 
                these kind of mechanism can be used to interpret the result of other machine learning techniques

                For example you can use it to interpret the solutions of clustering
                    in the moment you get the cluster is very difficult to understand if it is meaningful or not 
                    we try to interpret the clustering output as a label of a supervised learning dataset, the label is the cluster to which each sample is assigned 
                    youre going to have a decision tree that tries to explain 
                    and so you can understand if the cluster is meaning or not 

                manca ovviamente una parte 
        
    Neural Networks
        Neural networks are mathematical representations
        inspired by the functioning of the human brain”
        Neural networks can model very complex patterns and
        decision boundaries in the data.

        However if we analyse how a NN works, in truth they are just generalisation of logistic regression 

        because a neuron uses sigmoid as activating function 

        logistic regression is a neural network with just a neuron (contestabile)


        MultiLayer Perceptron 
            sequence of neurons organised in different layers 
                input : acquires the input
                hidden : number of neurons that perform feature extraction, try to combine the feature inputs with the weights assigned to the different logistic functions 
                output : number of nodes depending on the result we want to obtain
            
            For classification: logistic transform in the output layer 
            For regression: you can use a linear function 

        Details 
            in fraud analytics no complex patterns, good with just one hidden layer 
            Universal Approximators, in the fraud domain they're able to approximate any function 
            no complex models 

        Weight Learning:
            to predict continuous target variable MSE 
            binary target variable MAXIMUM LIKELIHOOD 

            start from random weights, then start an optimization proble 
            perform until you reach the solution or the limit of the number of operations 

            The function they try to minimize is not convex so it has multiple local minima but only one global minimum

            You can mitigate the fact to remain in a local minima with :
                preliminary training: start from different sets with random weights and select the one with best performance 
                stopping criterion: when no more progress, weights stop changing substantially, after a certain number of epochs 
        
        Second problem: select the number of layers and hidden neurons per each layer 
            Select them based on the complexity and non-linearity of the patterns in data 
            You want to find more complex relations between input and output 

            Do train,val,test split and do things 

        Overfitting Problem 
            13:30
            Options to mitigate overfitting 
             1: train test and stop training when you notice an increase of classificaiton error 
             2: weight regularization: put some bound on the value of each weight is going to assume, because 
                                        nn work updating weights associated to hidden neurons, so bigger weights can 
                                        make the model give importance to something which is not so good 
        
        13:33 esempio di domanda esame 
        Disadvantage è che sono black box method 
            impossible or not easy to interpret, it may make investigation longer 

            But we have a way to interpret them (capisci che cosa ha detto)
                hinton diagram 
                    how many hidden neurons you've activated?
                    look at the size of the weights (these two neurons will be activated when a thing arrives to input)
                    in the example do not consider the income because it is activated by small number of neurons with small weights
                    remove it and 

                Another way is to use backward variable selection 
                remove features until you start to see a decrease of performance 

                The two methodologies above allows us to perform variable selection 
                but ??? 13:43

            Rule Extraction Procedure 
                you can use 
                Decompositional Technique: decompose the NN internal workings by automatically analysing weights and/or activation values 
                Pedagogical Technique: vedila 

                Example:
                    + parte finale lezione vedi tutto 

                    

\fi


        








