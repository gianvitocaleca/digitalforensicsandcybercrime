    \subsection{Support Vector Machines}
        Method that originates from linear programming:
        \begin{itemize}
            \item Objective function 
            \item Constraints 
        \end{itemize}
        SVMs aim at maximizing the margin to pull both classes as far apart as possible.\\
        We call \textbf{support vectors} the training points that lie on the separating hyperplanes.\\
        The optimization problem has a quadratic cost function, no local minima and only one global minimum.\\
        In the real world it exists the non-linear separable case, add an error term to consider how much are we misclassifying, use kernel function to change feature space.
        \subsubsection{Interpretability and Variable Selection}
            They are black box methods, complex in settings where interpretability is important.\\
            Variable selection can be performed using backward variable selection (this reduces the variables but not provide any additional insight into the workings of the SVM).
        \subsubsection{Rule Extraction}
            \begin{itemize}
                \item \textbf{Decompositional:} represent SVM as a neural network
                \item \textbf{Pedagogical:} 
                \begin{itemize}
                    \item Use SVM first to construct a dataset with SVM predictions for each one of the observations
                    \item Give the dataset to a decision tree algorithm to build a decision tree
                \end{itemize}
                \item \textbf{Two stage models:} A simple model is estimated first, followed by a SVM to correct the errors of the latter
            \end{itemize}
    \subsection{Ensemble Methods}
        The idea is to provide an explanation of the target label by putting toghether different models combining their results, based on the assumption that multiple diverse models capture different trends of the dataset.\\
        To be successful, ensemble must be done with models sensitive to changes.
        \begin{itemize}
            \item \textbf{Bagging}
            \item \textbf{Boosting}
            \item \textbf{Random Forests}
        \end{itemize}
        \subsubsection{Bagging}
            \begin{itemize}
                \item Start by taking B bootstraps from the underlying sample. A bootstrap is a sample with replacement.
                \item Build a model for every bootstrap
                \item Use majority voting for classification, average for regression
            \end{itemize}
            The key element for bagging is the instability of the analytical technique. For models that are robust with respect to the underlying dataset, bagging will not give much added value.
        \subsubsection{Boosting}
            Estimate multiple models using a weighted data sample \textit{(uniform at the beginning)}.\\
            Iteratively re-weight data according to classification error.\\
            The idea is that difficult observations should get more attention. The final ensemble is a weighted combination of all the individual models.
            \begin{itemize}
                \item Key Advantage: easy to implement
                \item Potential Drawback: risk to overfitting to the hard \textit{(potentially noisy)} examples in the data, which will get higher weights as the algorithm proceeds. Relevant in fraud detection setting because the target labels are tipically quite noisy.
            \end{itemize}
        \subsubsection{Random Forests}
            Creates a forest of decision trees:
            \begin{itemize}
                \item Given a dataset with n observations and N inputs 
                \item m = constant chosen on beforehand 
                \item For t = 1,\dots,T:
                \begin{itemize}
                    \item take a bootstrap sample with n observations 
                    \item build a decision tree whereby for each node of the tree, randomly choose m variables on which to base the splitting decision
                    \item Split on the best of this subset 
                    \item Fully grow each tree without pruning
                \end{itemize}
            \end{itemize}
            Create as much diversity in the classifiers, the higher the diversity, the higher the performances.
        \subsubsection{Evaluating Random Forests}
            Random forests can achieve excellent predictive performances.\\
            Their main disadvantage is that they are black-box models because they're based on random decision trees.
\section{Evaluating a Fraud Detection Model}
\iffalse
Evaluating a Fraud Detection Model
    Two main phases:
        How to split up the dataset 
        Which performance metrics I have to consider 
    
    Splitting of the dataset:
        If large dataset: train, validation, test with no data shared, otherwise you're cheating
            strict separation between them 
            perform stratified sampling (each split must be representative of the dataset)
        
        Small dataset:
            cross validation: try to mitigate the problem of having a small dataset 
                small dataset but not so small 
                divide the dataset in k parts (folds)
                build a model training on k-1 folds and testing in the last one fold 
                then go on until you tested on all folds 
                final performance will be an average of the performance on each of the datasets 
                here you are somehow randomizing the parts to see if the model is really generalizing
            leave-one-out: very small
                train on N-1, test on 1 sample.
            
            Problem:
                cross validation gives us multiple models, 
                    present the results of cross validation 
                    then to provide a model you can build a model based on your entire dataset, re-train 
                    or, build an ensemble of all the models 
                    in the case of LOO, you can pick one at random because the difference is of only one sample.
    
    Performance Metrics 
        confusion matrix 
        based on it you can compute
        classification accuracy
        classification error
        sensitivity, recall, hit rate : how many true positive were you able to correctly detect
        specificity
        precision

        usually accuracy is not a good measure in fraud detection domain
        because frauds are rare, disperse in a huge amount of data 
        they represent 0.1 percent of data 
        if you say that everything is legitimate you will get an accuracy of 99,9

        With an unbalance dataset, accuracy is not good 
        the good ones are F1 measure: armonic mean of precision and recall 
        Matthew Correlation Coefficient
        or other graphical measures (we will see)
        and precision/recall 

        All the performance measures will be dependent on the treshold 
        0= nothing is fraud 
        1= everything is fraud

        Receiving Operating Characteristic Curve 
         13:15
         a good model will be near to the extreme left point of the graph 
        the more you move near the position 
        the more in the diagonal the more random model performance 

        problem: if you have few models to compare, okay 
                to compare very different models, it's difficult 

        Area under The ROC curve 
            solution to the problem (13:17)
        
    Developing predictive models for skewed data sets 
        the datasets will be full of non-positive data examples

        to manage this there are two ways:
            differently weight the samples (higher to fraudulent cases)

            rebalance the dataset by oversampling the minority case or by undersampling the majority case 

            Or decide to use a larger dataset 13:19 circa 

                main challenge of performing under-over sampling is that you have to decide
                    how much to over-under sampling 
                    this will impact the final performance 
                    this methodology must be applied only on the training set and not on the test set 

                    the optimal number of over-under sampling 
                    be much as possible near to the distribution
                        but it really depends on the case 
                            select the one that achieves the best results 
                        bu under-over you introduce bias 

                        empirical procedure 
                            start from bla 
                            poi bla bla 

                        in general it seems that undersampling works better than oversampling 
                
                another technique is: syntetic mynority oversampling technique
                    syntetically generate new samples from the one available
                    identify the k nearest neighbors from the sample you want to replicate
                    and then syntetically generate the new one by average the neighbors
                    so you generated a new one which is slightly different 

        Cost-Sensitive Learning 
            besides interpretability, efficiency, main problem:
                your output if is not in real time it's going to be investigated 
                investigation 
                you can estimate a cost associated to the misclassification,
                in case of misclassification, boooh ultima cosa bro.

                quando dice estimate this cost il cost Ã¨ quello total ma poi lui ha puntato ai false positives

                the same metric can be used to apply many theoretical procedures 

                Some example of question at the exam.
     \fi           
