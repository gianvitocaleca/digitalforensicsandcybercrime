\chapter{Machine Learning for Fraud Detection}
    We will discuss a selection of techniques with a particular focus on the fraud practitioner's perspective
    \section{Data Preprocessing Step}
        Data are tipically dirty. \textbf{Data-filter mechanisms} must be applied to clean up and reduce the data, because even the littlemost mistake can make the data totally unusable and lead to a faulty model.
        \subsection{Types of data sources}
            When building a data driven model you may use data from different sources that provide different types of information. We will consider transactional data.
        \subsection{Transactional data}
            \textit{Structured and detailed information capturing the key characteristics of a customer transaction}
            Usually summarized over long time horizons by \textbf{aggregating}:
            \begin{itemize}
                \item averages
                \item trends 
                \item maximum or minimum values
            \end{itemize}
            On these data, the features you want to keep are:
            \begin{itemize}
                \item \textbf{Amount of money}
                \item \textbf{Frequency}
                \item \textbf{Recency}
            \end{itemize}
            These three features are meaningful when interpreted individually, and their interaction is very useful for fraud detection and anti-money laundering.\\
            All these features come from expert driven analysis of the data.
        \subsection{Merging data sources}
            Once the correct sources are selected, them must be merged in a structured manner (i.e. a table):
            The rows represent the entities to analyze, while the columns contain the information about the entities.
        \subsection{Types of data elements}
            \begin{itemize}
                \item \textbf{Continous:} data elements defined on an interval which can be limited or unlimited
                \item \textbf{Categorical:}
                \begin{itemize}
                    \item \textit{Nominal:} can only take on a limited set of values with no meaningful ordering in between
                    \item \textit{Ordinal:} can only take on a limited set of values, but with a meaningful ordering in between
                    \item \textit{Binary:} can only take one out of two values
                \end{itemize}
            \end{itemize}
            In the fraud detection domain data usually are a mixture of the three kinds of categorical types.\\
            Preprocessing is also needed to transform all the sources in a way to make them understandable by the model.
        \subsection{Sampling}
            It is the process to take a subset of historical data to build an analytical model. Usually models are built on samples of the entire dataset,
            much smaller than the whole dataset. Why don't we use the whole dataset?
            \begin{itemize}
                \item Data must be representative for the future entities, so maybe lots of data which are not recent can cause lack in representation.
                \item Some features never change, maybe a small dataset would represent the same information of the whole one.
            \end{itemize}
            \subsubsection{Sampling timing and bias}
                Trade off between:
                \begin{itemize}
                    \item Lots of data (more robust model)
                    \item Recent data (more representative model)
                \end{itemize}
                Sampling can introduce bias:\\
                Which month is the more representative? (different behavior in december or february).
                Every month may deviate from the norm (average), two possible solutions:
                \begin{itemize}
                    \item \textbf{Build separate models:}  for different months or homogeneous time-frames, complex solution, multiple models to manage.
                    \item \textbf{Build a single model:} Sample observations over a period covering a full business cycle, reduced power since less tailoring on particular time frames, but lower complexity and costs.
                \end{itemize}
            \subsubsection{Stratified sampling:}
                In a fraud detection context data sets are very skew:
                \begin{itemize}
                    \item Stratify according to the target fraud indicator to have samples containing exactly the same percentage of fraudulent/non-fraudulent transaction as in the original data
                    \item Strafity according to predictor variables: resemble the real product transaction distribution
                \end{itemize}
                Notice that sampling may also introduce bias according to the considered subset of data.